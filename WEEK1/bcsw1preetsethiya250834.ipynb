{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Text Preprocessing**"
      ],
      "metadata": {
        "id": "IYACLCwsyxfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Data-** given in the below code cell\n",
        "\n",
        "**1.1: Preprocessing From Scratch**\n",
        "\n",
        "**Goal:** Write a function clean_text_scratch(text) that performs the following without using NLTK or Spacy:\n",
        "\n",
        "1. Lowercasing: Convert text to lowercase.\n",
        "\n",
        "2. Punctuation Removal: Use Python's re (regex) library or string methods to remove special characters (!, ., ,, :, ;, ..., ').\n",
        "\n",
        "3. Tokenization: Split the string into a list of words based on whitespace.\n",
        "\n",
        "4. Stopword Removal: Filter out words found in this list: ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'].\n",
        "\n",
        "5. Simple Stemming: Create a helper function that removes suffixes 'ing', 'ly', 'ed', and 's' from the end of words.\n",
        "\n",
        "\n",
        "Note: This is a \"Naive\" stemmer. It will break words like \"sing\" -> \"s\". This illustrates why we need libraries!\n",
        "\n",
        "**Task:** Run this function on the first sentence of the corpus and print the result."
      ],
      "metadata": {
        "id": "MTP8EqylwqDf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "933aeiXfP1Dz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qIRv3qS2bTFt"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "# This is our dataset (The Corpus)\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "\n",
        "def clean_text_scratch(text):\n",
        "    \"\"\"\n",
        "    Manually cleans text by lowercasing, removing punctuation,\n",
        "    removing stopwords, and performing simple stemming.\n",
        "    \"\"\"\n",
        "    # 1. Lowercasing\n",
        "    # We convert everything to lowercase to ensure \"The\" and \"the\" are treated the same.\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Punctuation Removal\n",
        "    # We use regex to replace any character that is NOT a word character (\\w)\n",
        "    # or a whitespace (\\s) with an empty string.\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # 3. Tokenization\n",
        "    # Split the string by spaces to get a list of individual words.\n",
        "    words = text.split()\n",
        "\n",
        "    # 4. Stopword Removal\n",
        "    # These are common words that usually don't add much meaning to the topic.\n",
        "    stop_words = ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or']\n",
        "\n",
        "    # We create a new list containing only words NOT in our stop_words list.\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # 5. Simple Stemming Helper Function\n",
        "    def simple_stem(word):\n",
        "        # We check for suffixes and remove them.\n",
        "        # Order matters! We usually check longer suffixes first to avoid partial cuts.\n",
        "        if word.endswith('ing'):\n",
        "            return word[:-3] # Remove last 3 chars\n",
        "        elif word.endswith('ly'):\n",
        "            return word[:-2] # Remove last 2 chars\n",
        "        elif word.endswith('ed'):\n",
        "            return word[:-2]\n",
        "        elif word.endswith('s'):\n",
        "            return word[:-1]\n",
        "        return word\n",
        "\n",
        "    # Apply the stemmer to every word in our filtered list\n",
        "    stemmed_words = [simple_stem(word) for word in filtered_words]\n",
        "\n",
        "    return stemmed_words\n",
        "\n",
        "# Task Execution\n",
        "# We take the first sentence (index 0) and pass it through our function.\n",
        "first_sentence = corpus[0]\n",
        "result_scratch = clean_text_scratch(first_sentence)\n",
        "\n",
        "print(\"Original Sentence:\", first_sentence)\n",
        "print(\"Processed (Scratch):\", result_scratch)\n",
        "\n",
        "\n",
        "\n",
        "#write rest of the code here"
      ],
      "metadata": {
        "id": "oR4BKqITy17z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ee6758e-e518-46b0-a4c0-54cfef50794d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: Artificial Intelligence is transforming the world; however, ethical concerns remain!\n",
            "Processed (Scratch): ['artificial', 'intelligence', 'transform', 'world', 'however', 'ethical', 'concern', 'remain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2: Preprocessing Using Tools**\n",
        "\n",
        "**Goal:** Use the nltk library to perform the same cleaning on the entire corpus.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Use nltk.tokenize.word_tokenize.\n",
        "2. Use nltk.corpus.stopwords.\n",
        "3. Use nltk.stem.WordNetLemmatizer\n",
        "\n",
        "to convert words to their root (e.g., \"jumps\" $\\to$ \"jump\", \"transforming\" $\\to$ \"transform\").\n",
        "\n",
        "\n",
        "**Task:** Print the cleaned, lemmatized tokens for the second sentence (The pizza review)."
      ],
      "metadata": {
        "id": "dN9rNq7WycqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#write rest of the code here\n",
        "# --- The Main Function ---\n",
        "def clean_text_nltk(text):\n",
        "    \"\"\"\n",
        "    Cleans text using NLTK tools.\n",
        "    It's smarter than our manual version because it understands English grammar.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Initialize the Lemmatizer\n",
        "    # Think of this as a \"Smart Dictionary\" lookup tool.\n",
        "    # Unlike our manual stemmer that blindly chopped off 'ing', this looks up the actual root word.\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # 2. Smart Tokenization\n",
        "    # In 1.1, we used text.split() which just cuts at spaces.\n",
        "    # word_tokenize is smarter. It knows that \"won't\" should be split into \"wo\" and \"n't\",\n",
        "    # and it handles punctuation better than just deleting it.\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 3. Load the Stopwords List\n",
        "    # NLTK has a pre-made list of 179 english stopwords (much more than our manual list of 10).\n",
        "    # We convert it to a 'set' because checking if a word exists in a set is faster than a list.\n",
        "    english_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "    cleaned_tokens = []\n",
        "\n",
        "    # Loop through every word (token) we found\n",
        "    for token in tokens:\n",
        "        # Convert to lowercase so \"The\" matches \"the\"\n",
        "        token_lower = token.lower()\n",
        "\n",
        "        # Check two things:\n",
        "        # A. isalnum(): Is it an alphabet or number? (Removes punctuation like '...' or '!')\n",
        "        # B. not in stopwords: Is it a meaningful word?\n",
        "        if token_lower.isalnum() and token_lower not in english_stopwords:\n",
        "\n",
        "            # 4. Lemmatization (The Magic Step)\n",
        "            # We ask the lemmatizer: \"What is the root verb (action) of this word?\"\n",
        "            # pos='v' stands for Part-Of-Speech = Verb.\n",
        "            # If we don't say pos='v', it assumes everything is a Noun.\n",
        "            # This ensures \"was\" becomes \"be\" and \"transforming\" becomes \"transform\".\n",
        "            lemma = lemmatizer.lemmatize(token_lower, pos='v')\n",
        "\n",
        "            # Add the clean, root word to our final list\n",
        "            cleaned_tokens.append(lemma)\n",
        "\n",
        "    return cleaned_tokens\n",
        "\n",
        "# --- Task Execution ---\n",
        "# We are asked to process the SECOND sentence (index 1 is the Pizza review).\n",
        "second_sentence = corpus[1]\n",
        "result_nltk = clean_text_nltk(second_sentence)\n",
        "\n",
        "print(\"Original Sentence:\", second_sentence)\n",
        "print(\"-\" * 50)\n",
        "print(\"Processed (NLTK):\", result_nltk)"
      ],
      "metadata": {
        "id": "v_4FjuCqy5Kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cc35f06-df25-4581-bf01-c5b31de3cd5e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: The pizza was absolutely delicious, but the service was terrible ... I won't go back.\n",
            "--------------------------------------------------\n",
            "Processed (NLTK): ['pizza', 'absolutely', 'delicious', 'service', 'terrible', 'wo', 'go', 'back']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Text Representation**"
      ],
      "metadata": {
        "id": "hPMrwva2y1LG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1: Bag of Words (BoW)**\n",
        "\n",
        "**Logic:**\n",
        "\n",
        "**Build Vocabulary:** Create a list of all unique words in the entire corpus (after cleaning). Sort them alphabetically.\n",
        "\n",
        "**Vectorize:** Write a function that takes a sentence and returns a list of numbers. Each number represents the count of a vocabulary word in that sentence.\n",
        "\n",
        "**Task:** Print the unique Vocabulary list. Then, print the BoW vector for: \"The quick brown fox jumps over the lazy dog.\""
      ],
      "metadata": {
        "id": "cKa8NnZ5zLlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# NLTK downloads (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# rest of the code here\n",
        "# Define our corpus again just to be safe\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "\n",
        "# --- Helper Function: Clean text using NLTK ---\n",
        "# We use this because the notebook cell imported these libraries for us!\n",
        "def nltk_preprocess(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    clean_words = []\n",
        "    for token in tokens:\n",
        "        token_lower = token.lower()\n",
        "        if token_lower.isalnum() and token_lower not in stop_words:\n",
        "            # Convert to root word (lemmatize)\n",
        "            lemma = lemmatizer.lemmatize(token_lower, pos='v')\n",
        "            clean_words.append(lemma)\n",
        "    return clean_words\n",
        "\n",
        "# --- Step 1: Build Vocabulary ---\n",
        "all_words = []\n",
        "for sentence in corpus:\n",
        "    words = nltk_preprocess(sentence)\n",
        "    all_words.extend(words)\n",
        "\n",
        "# Create unique sorted vocabulary\n",
        "vocabulary = sorted(list(set(all_words)))\n",
        "\n",
        "print(\"--- Unique Vocabulary List ---\")\n",
        "print(vocabulary)\n",
        "\n",
        "# --- Step 2: Vectorize (The Manual Logic) ---\n",
        "def get_bow_vector(sentence, vocab):\n",
        "    # 1. Clean the input sentence using the same NLTK method\n",
        "    sentence_words = nltk_preprocess(sentence)\n",
        "\n",
        "    # 2. Create a vector of zeros\n",
        "    vector = [0] * len(vocab)\n",
        "\n",
        "    # 3. Count the words\n",
        "    for word in sentence_words:\n",
        "        if word in vocab:\n",
        "            index = vocab.index(word)\n",
        "            vector[index] += 1\n",
        "    return vector\n",
        "\n",
        "# --- Task Execution ---\n",
        "test_sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "bow_vector = get_bow_vector(test_sentence, vocabulary)\n",
        "\n",
        "print(\"\\n--- BoW Vector for Test Sentence ---\")\n",
        "print(f\"Sentence: '{test_sentence}'\")\n",
        "print(\"Vector:\", bow_vector)\n"
      ],
      "metadata": {
        "id": "yVUFCkm7yrg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db12e565-e023-432d-f157-c25559958477"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Unique Vocabulary List ---\n",
            "['absolutely', 'algebra', 'artificial', 'back', 'behind', 'brown', 'concern', 'data', 'delicious', 'dog', 'ethical', 'fox', 'go', 'hate', 'however', 'intelligence', 'involve', 'jump', 'lazy', 'learn', 'linear', 'love', 'machine', 'math', 'mind', 'nobler', 'pizza', 'question', 'quick', 'remain', 'science', 'service', 'statistics', 'terrible', 'transform', 'whether', 'wo', 'world']\n",
            "\n",
            "--- BoW Vector for Test Sentence ---\n",
            "Sentence: 'The quick brown fox jumps over the lazy dog.'\n",
            "Vector: [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2: BoW Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.CountVectorizer.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Instantiate the vectorizer.\n",
        "\n",
        "2. fit_transform the raw corpus.\n",
        "\n",
        "3. Convert the result to an array (.toarray()) and print it."
      ],
      "metadata": {
        "id": "UwsoZix-zUDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#rest of the code here\n",
        "# --- Step 1: Instantiate the Vectorizer ---\n",
        "\n",
        "# We don't need to write loops to count words; this object does it for us.\n",
        "# It automatically handles lowercasing and basic tokenization.\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# --- Step 2: Learn and Transform ---\n",
        "# fit_transform() does two jobs at once:\n",
        "# 1. 'Fit': Reads all sentences to build the master Vocabulary (the dictionary).\n",
        "# 2. 'Transform': Converts the sentences into matrices of numbers.\n",
        "# 'X' is our raw data converted into math.\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# --- Step 3: Inspect the Output ---\n",
        "# The output 'X' is a \"Sparse Matrix\".\n",
        "# Since most words don't appear in most sentences, the matrix is mostly empty (zeros).\n",
        "# To save RAM, Python compresses it. We use .toarray() to uncompress it so we can see it.\n",
        "\n",
        "print(\"--- The Vocabulary (Feature Names) ---\")\n",
        "# This shows us which column corresponds to which word (e.g., Column 0 might be 'absolutely')\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\n--- The Document-Term Matrix (The Counts) ---\")\n",
        "# Rows = Sentences, Columns = Words\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "id": "RGs7EzLRzfGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc7bedf2-6997-40d0-a2a2-a946c9b59d86"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- The Vocabulary (Feature Names) ---\n",
            "['absolutely' 'algebra' 'and' 'artificial' 'back' 'be' 'behind' 'brown'\n",
            " 'but' 'concerns' 'data' 'delicious' 'dog' 'ethical' 'fox' 'go' 'hate'\n",
            " 'however' 'in' 'intelligence' 'involves' 'is' 'it' 'jumps' 'lazy'\n",
            " 'learning' 'linear' 'love' 'machine' 'math' 'mind' 'nobler' 'not' 'or'\n",
            " 'over' 'pizza' 'question' 'quick' 'remain' 'science' 'service'\n",
            " 'statistics' 'terrible' 'that' 'the' 'tis' 'to' 'transforming' 'was'\n",
            " 'whether' 'won' 'world']\n",
            "\n",
            "--- The Document-Term Matrix (The Counts) ---\n",
            "[[0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1]\n",
            " [1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 0 1 0 1 0 2 0 0 0 2 0 1 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0\n",
            "  0 1 0 0 0 0 0 0 2 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0\n",
            "  1 0 0 0 0 0 0 1 2 1 2 0 0 1 0 0]\n",
            " [0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3: TF-IDF From Scratch (The Math)**\n",
        "\n",
        "**Goal:** Manually calculate the score for the word \"machine\" in the last sentence:\n",
        "\n",
        "\"I love machine learning, but I hate the math behind it.\"\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "*TF (Term Frequency):* $\\frac{\\text{Count of 'machine' in sentence}}{\\text{Total words in sentence}}$\n",
        "\n",
        "*IDF (Inverse Document Frequency):* $\\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing 'machine'}})$ (Use math.log).\n",
        "\n",
        "**Result:** TF * IDF.\n",
        "\n",
        "**Task:** Print your manual calculation result."
      ],
      "metadata": {
        "id": "-MR6Bxgh0Gpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write code here\n",
        "import math\n",
        "import re\n",
        "\n",
        "# We focus on the last sentence: \"I love machine learning, but I hate the math behind it.\"\n",
        "target_sentence_raw = corpus[5]\n",
        "target_word = \"machine\"\n",
        "\n",
        "# --- Pre-processing ---\n",
        "# Just a quick clean to ensure we count \"Machine\" and \"machine\" as the same.\n",
        "clean_sentence = re.sub(r'[^\\w\\s]', '', target_sentence_raw.lower())\n",
        "word_list = clean_sentence.split()\n",
        "\n",
        "print(f\"Analyzing Sentence: '{clean_sentence}'\")\n",
        "print(f\"Target Word: '{target_word}'\")\n",
        "\n",
        "# --- Step 1: Calculate TF (Term Frequency) ---\n",
        "# Question: How important is this word in THIS specific sentence?\n",
        "# Formula: (Count of 'machine' here) / (Total words here)\n",
        "\n",
        "count_in_sentence = word_list.count(target_word)\n",
        "total_words = len(word_list)\n",
        "\n",
        "tf = count_in_sentence / total_words\n",
        "print(f\"\\n1. TF Calculation: {count_in_sentence} / {total_words} = {tf:.4f}\")\n",
        "\n",
        "# --- Step 2: Calculate IDF (Inverse Document Frequency) ---\n",
        "# Question: How rare is this word across ALL documents?\n",
        "# Logic: If a word appears everywhere (like 'the'), it's not unique. We want rare words.\n",
        "\n",
        "total_documents = len(corpus)\n",
        "docs_with_word = 0\n",
        "\n",
        "# Scan all sentences to see how many contain 'machine'\n",
        "for doc in corpus:\n",
        "    # Quick clean and check\n",
        "    if target_word in re.sub(r'[^\\w\\s]', '', doc.lower()).split():\n",
        "        docs_with_word += 1\n",
        "\n",
        "# Formula: log( Total Docs / Docs with Word )\n",
        "# We use log because otherwise, the numbers get too big too fast.\n",
        "idf = math.log(total_documents / docs_with_word)\n",
        "print(f\"2. IDF Calculation: log({total_documents} / {docs_with_word}) = {idf:.4f}\")\n",
        "\n",
        "# --- Step 3: Final TF-IDF Score ---\n",
        "tfidf = tf * idf\n",
        "\n",
        "print(f\"\\n--- Final Result ---\")\n",
        "print(f\"TF-IDF Score for '{target_word}': {tfidf:.4f}\")"
      ],
      "metadata": {
        "id": "gNSo-nza0k_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6db8f4bf-f519-4fdb-ac7e-48a440877571"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing Sentence: 'i love machine learning but i hate the math behind it'\n",
            "Target Word: 'machine'\n",
            "\n",
            "1. TF Calculation: 1 / 11 = 0.0909\n",
            "2. IDF Calculation: log(6 / 2) = 1.0986\n",
            "\n",
            "--- Final Result ---\n",
            "TF-IDF Score for 'machine': 0.0999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4: TF-IDF Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.TfidfVectorizer.\n",
        "\n",
        "**Steps:** Fit it on the corpus and print the vector for the first sentence.\n",
        "\n",
        "**Observation:** Compare the score of unique words (like \"Intelligence\") vs common words (like \"is\"). Which is higher?"
      ],
      "metadata": {
        "id": "YEYkuoSb0nDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# rest of the code here\n",
        "import pandas as pd # Importing pandas just to make the output look like a nice table\n",
        "\n",
        "# --- Step 1: Instantiate ---\n",
        "# This tool does everything we did in Part 2.3 automatically.\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# --- Step 2: Create the Matrix ---\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# --- Step 3: Analysis (Let's look at Sentence 0) ---\n",
        "# We extract the vector for the first sentence (\"Artificial Intelligence...\")\n",
        "first_sentence_vector = tfidf_matrix[0]\n",
        "\n",
        "# We get the list of words\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create a DataFrame to view scores cleanly\n",
        "df = pd.DataFrame(first_sentence_vector.T.todense(), index=feature_names, columns=[\"TF-IDF Score\"])\n",
        "\n",
        "# Sort by highest score first\n",
        "sorted_df = df.sort_values(by=[\"TF-IDF Score\"], ascending=False)\n",
        "\n",
        "print(\"--- Top Important Words in Sentence 1 ---\")\n",
        "# We filter out words with 0 score (words that aren't in the sentence)\n",
        "print(sorted_df[sorted_df[\"TF-IDF Score\"] > 0])\n",
        "\n",
        "# Observation: Notice how 'intelligence' has a higher score than 'is'.\n",
        "# The model knows 'is' is common trash, but 'intelligence' is rare gold."
      ],
      "metadata": {
        "id": "Of6PfWyd0pnl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f83dce0-a0ee-4a18-adc9-142e4a67415c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Top Important Words in Sentence 1 ---\n",
            "              TF-IDF Score\n",
            "artificial        0.334545\n",
            "however           0.334545\n",
            "intelligence      0.334545\n",
            "concerns          0.334545\n",
            "ethical           0.334545\n",
            "remain            0.334545\n",
            "world             0.334545\n",
            "transforming      0.334545\n",
            "is                0.274332\n",
            "the               0.171397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3- Word Embeddings**"
      ],
      "metadata": {
        "id": "YWAar8IIzp_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1: Word2Vec Using Tools**\n",
        "\n",
        "**Task:** Train a model using gensim.models.Word2Vec.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Pass your cleaned tokenized corpus (from Part 1.2) to Word2Vec.\n",
        "\n",
        "2. Set min_count=1 (since our corpus is small, we want to keep all words).\n",
        "\n",
        "3. Set vector_size=10 (small vector size for easy viewing).\n",
        "\n",
        "**Experiment:** Print the vector for the word \"learning\"."
      ],
      "metadata": {
        "id": "uY1URFxgz036"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#rest of the code here\n",
        "# --- Step 0: Setup NLTK (The Brains) ---\n",
        "# We need to make sure we have the data for the libraries we just imported.\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "# Our Dataset\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "\n",
        "# --- Step 1: Preprocessing Function ---\n",
        "# Since the assignment imported Stopwords and Lemmatizer, we MUST use them.\n",
        "# This cleans the text so the model learns \"jump\" instead of \"jumps\".\n",
        "def clean_for_word2vec(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    clean_tokens = []\n",
        "    for token in tokens:\n",
        "        token_lower = token.lower()\n",
        "        if token_lower.isalnum() and token_lower not in stop_words:\n",
        "            # Convert to root word (Verb)\n",
        "            lemma = lemmatizer.lemmatize(token_lower, pos='v')\n",
        "            clean_tokens.append(lemma)\n",
        "    return clean_tokens\n",
        "\n",
        "# --- Step 2: Prepare Data ---\n",
        "# We apply our cleaning function to every sentence.\n",
        "# Result is a list of lists: [['artificial', 'intelligence', 'transform', ...], ...]\n",
        "cleaned_corpus = [clean_for_word2vec(sentence) for sentence in corpus]\n",
        "\n",
        "# --- Step 3: Train the Model ---\n",
        "# sentences: The cleaned data\n",
        "# vector_size=10: Each word is a list of 10 numbers\n",
        "# min_count=1: Keep words even if they appear only once (because our data is small)\n",
        "# window=5: Look 5 words left and right for context\n",
        "model = Word2Vec(sentences=cleaned_corpus, vector_size=10, min_count=1, window=5)\n",
        "\n",
        "# --- Step 4: Inspect the Brain ---\n",
        "target_word = \"learning\"\n",
        "\n",
        "if target_word in model.wv:\n",
        "    vector = model.wv[target_word]\n",
        "    print(f\"--- Vector representation for '{target_word}' ---\")\n",
        "    print(vector)\n",
        "    print(f\"\\nVector Shape: {vector.shape}\")\n",
        "else:\n",
        "    # Note: If 'learning' was a stopword (it's not), this would trigger.\n",
        "    print(f\"The word '{target_word}' was cleaned out or not found.\")\n"
      ],
      "metadata": {
        "id": "aziX2IGBzyaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a9f222f-823f-4033-aee6-8652170ad65b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "The word 'learning' was cleaned out or not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3: Pre-trained GloVe (Understanding Global Context)**\n",
        "\n",
        "**Task:** Use gensim.downloader to load 'glove-wiki-gigaword-50'\n",
        "\n",
        "**Analogy Task:** Compute the famous analogy:$\\text{King} - \\text{Man} + \\text{Woman} = ?$\n",
        "\n",
        "Use model.most_similar(positive=['woman', 'king'], negative=['man']).\n",
        "\n",
        "**Question:** Does the model correctly guess \"Queen\"?"
      ],
      "metadata": {
        "id": "r3J42eQZ1fUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "\n",
        "import gensim.downloader as api\n",
        "\n",
        "# --- Step 1: Load the Giant Brain ---\n",
        "# We are downloading a model called 'glove-wiki-gigaword-50'.\n",
        "# 'glove': Global Vectors for Word Representation.\n",
        "# 'wiki-gigaword': Trained on Wikipedia and Gigaword (news).\n",
        "# '50': Each word is a list of 50 numbers (Vector Size).\n",
        "\n",
        "print(\"Loading pre-trained GloVe model... (This might take 1-2 minutes)\")\n",
        "glove_model = api.load('glove-wiki-gigaword-50')\n",
        "\n",
        "# --- Step 2: The Analogy Task ---\n",
        "# We want to solve: King - Man + Woman = ?\n",
        "# In Gensim, we express this as:\n",
        "# Positive contribution: ['king', 'woman'] (Add these vectors)\n",
        "# Negative contribution: ['man'] (Subtract this vector)\n",
        "\n",
        "try:\n",
        "    # most_similar() finds the word whose vector is closest to the result of our math.\n",
        "    result = glove_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "\n",
        "    # The result comes back as a list of tuples: [('queen', 0.8523...)]\n",
        "    top_match = result[0][0]     # The word itself ('queen')\n",
        "    confidence = result[0][1]    # How sure the model is (0.0 to 1.0)\n",
        "\n",
        "    print(\"\\n--- Analogy Result: King - Man + Woman ---\")\n",
        "    print(f\"The model guessed: {top_match}\")\n",
        "    print(f\"Confidence Score: {confidence:.4f}\")\n",
        "\n",
        "    # Logic Check:\n",
        "    # If the output is 'queen', the model understands gender roles!\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "# --- Extra Experiment (Optional) ---\n",
        "# Let's try: Paris - France + Italy = ? (Should be Rome)\n",
        "# city = glove_model.most_similar(positive=['italy', 'paris'], negative=['france'], topn=1)\n",
        "# print(f\"Paris - France + Italy = {city[0][0]}\")"
      ],
      "metadata": {
        "id": "LEj5SkO81mkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "813895a6-e2de-42dc-989f-07f350e9f50e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Loading pre-trained GloVe model... (This might take 1-2 minutes)\n",
            "\n",
            "--- Analogy Result: King - Man + Woman ---\n",
            "The model guessed: queen\n",
            "Confidence Score: 0.8524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 5- Sentiment Analysis (The Application)**\n",
        "\n",
        "**Concept:** Sentiment Analysis determines whether a piece of text is Positive, Negative, or Neutral. We will use VADER (Valence Aware Dictionary and sEntiment Reasoner) from NLTK. VADER is specifically designed for social media text; it understands that capital letters (\"LOVE\"), punctuation (\"!!!\"), and emojis change the sentiment intensity.\n",
        "\n",
        "**Task:**\n",
        "\n",
        "1. Initialize the SentimentIntensityAnalyzer.\n",
        "\n",
        "2. Pass the Pizza Review (corpus[1]) into the analyzer.\n",
        "\n",
        "3. Pass the Math Complaint (corpus[5]) into the analyzer.\n",
        "\n",
        "**Analysis:** Look at the compound score for both.\n",
        "\n",
        "**Compound Score Range:** -1 (Most Negative) to +1 (Most Positive).\n",
        "\n",
        "Does the model correctly identify that \"delicious\" and \"terrible\" in the same sentence result in a mixed or neutral score?"
      ],
      "metadata": {
        "id": "AbI4K0UJUxy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "# Download VADER (run once)\n",
        "#nltk.download('vader_lexicon')\n",
        "# rest of the code here\n",
        "# --- Step 1: Download the VADER Brain ---\n",
        "# VADER (Valence Aware Dictionary and sEntiment Reasoner) needs its own dictionary.\n",
        "# It knows words like \"love\" = +3.2 and \"hate\" = -2.7.\n",
        "nltk.download('vader_lexicon', quiet=True)\n",
        "\n",
        "# --- Step 2: Initialize the Analyzer ---\n",
        "# This creates the sentiment analysis object we will use.\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Let's pull the specific sentences from our corpus\n",
        "# Index 1: \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\"\n",
        "# Index 5: \"I love machine learning, but I hate the math behind it.\"\n",
        "pizza_review = corpus[1]\n",
        "math_complaint = corpus[5]\n",
        "\n",
        "def explain_sentiment(text):\n",
        "    print(f\"--- Analyzing: '{text}' ---\")\n",
        "\n",
        "    # .polarity_scores() returns a dictionary with 4 numbers:\n",
        "    # 'neg': Negative, 'neu': Neutral, 'pos': Positive\n",
        "    # 'compound': The overall score (-1 to +1)\n",
        "    scores = sia.polarity_scores(text)\n",
        "\n",
        "    print(f\"Scores: {scores}\")\n",
        "    print(f\"Compound Score: {scores['compound']}\")\n",
        "\n",
        "    # Logic to interpret the score\n",
        "    if scores['compound'] >= 0.05:\n",
        "        print(\"Verdict: Positive ðŸ˜„\")\n",
        "    elif scores['compound'] <= -0.05:\n",
        "        print(\"Verdict: Negative ðŸ˜ \")\n",
        "    else:\n",
        "        print(\"Verdict: Neutral ðŸ˜\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# --- Step 3: Run the Analysis ---\n",
        "explain_sentiment(pizza_review)\n",
        "explain_sentiment(math_complaint)\n",
        "\n",
        "# --- EXPERT OBSERVATION ---\n",
        "# Why is the pizza review score weird?\n",
        "# \"Delicious\" is Positive (+), but \"Terrible\" is Negative (-).\n",
        "# \"But\" is a logic flipper. VADER calculates the sum of these emotions.\n",
        "# Because \"won't go back\" is strongly negative, the compound score usually leans negative."
      ],
      "metadata": {
        "id": "_lC2c3GHUxU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bc1e31b-3ad8-42e3-c007-0e648814b0b3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Analyzing: 'The pizza was absolutely delicious, but the service was terrible ... I won't go back.' ---\n",
            "Scores: {'neg': 0.223, 'neu': 0.644, 'pos': 0.134, 'compound': -0.3926}\n",
            "Compound Score: -0.3926\n",
            "Verdict: Negative ðŸ˜ \n",
            "\n",
            "\n",
            "--- Analyzing: 'I love machine learning, but I hate the math behind it.' ---\n",
            "Scores: {'neg': 0.345, 'neu': 0.478, 'pos': 0.177, 'compound': -0.5346}\n",
            "Compound Score: -0.5346\n",
            "Verdict: Negative ðŸ˜ \n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}