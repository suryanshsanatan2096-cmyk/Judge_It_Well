{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Text Preprocessing**"
      ],
      "metadata": {
        "id": "IYACLCwsyxfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Data-** given in the below code cell\n",
        "\n",
        "**1.1: Preprocessing From Scratch**\n",
        "\n",
        "**Goal:** Write a function clean_text_scratch(text) that performs the following without using NLTK or Spacy:\n",
        "\n",
        "1. Lowercasing: Convert text to lowercase.\n",
        "\n",
        "2. Punctuation Removal: Use Python's re (regex) library or string methods to remove special characters (!, ., ,, :, ;, ..., ').\n",
        "\n",
        "3. Tokenization: Split the string into a list of words based on whitespace.\n",
        "\n",
        "4. Stopword Removal: Filter out words found in this list: ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'].\n",
        "\n",
        "5. Simple Stemming: Create a helper function that removes suffixes 'ing', 'ly', 'ed', and 's' from the end of words.\n",
        "\n",
        "\n",
        "Note: This is a \"Naive\" stemmer. It will break words like \"sing\" -> \"s\". This illustrates why we need libraries!\n",
        "\n",
        "**Task:** Run this function on the first sentence of the corpus and print the result."
      ],
      "metadata": {
        "id": "MTP8EqylwqDf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qIRv3qS2bTFt"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def clean_text_scratch(sentences):\n",
        "  sentences=sentences.lower()\n",
        "  tokens= sentences.split()\n",
        "  stopwords= ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or']\n",
        "  for word in tokens:\n",
        "    for words in stopwords:\n",
        "      if word==words:\n",
        "        tokens.remove(word)\n",
        "  clean=[]\n",
        "  for word in tokens:\n",
        "    if word.endswith(\"ing\"):\n",
        "      clean.append(word[:-3])\n",
        "    elif word.endswith(\"s\"):\n",
        "      clean.append(word[:-1])\n",
        "    elif word.endswith(\"ly\"):\n",
        "      clean.append(word[:-2])\n",
        "    elif word.endswith(\"ed\"):\n",
        "      clean.append(word[:-2])\n",
        "    elif word.endswith(\";\"):\n",
        "      clean.append(word[:-1])\n",
        "    elif word.endswith(\",\"):\n",
        "      clean.append(word[:-1])\n",
        "    elif word.endswith(\".\"):\n",
        "      clean.append(word[:-1])\n",
        "    elif word.endswith(\"!\"):\n",
        "      clean.append(word[:-1])\n",
        "    elif word.endswith(\"?\"):\n",
        "      clean.append(word[:-1])\n",
        "    elif word.endswith(\"..\"):\n",
        "      clean.append(word[:-1])\n",
        "    else:\n",
        "      clean.append(word)\n",
        "  return clean\n",
        "\n",
        "print(clean_text_scratch(corpus[0]))"
      ],
      "metadata": {
        "id": "oR4BKqITy17z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aed119e-9f9a-4661-ef02-d07e130ff383"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial', 'intelligence', 'transform', 'world', 'however', 'ethical', 'concern', 'remain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "\n",
        "cleaned_corpus = [re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", doc) for doc in corpus]"
      ],
      "metadata": {
        "id": "PXuUJjCPMi3P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2: Preprocessing Using Tools**\n",
        "\n",
        "**Goal:** Use the nltk library to perform the same cleaning on the entire corpus.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Use nltk.tokenize.word_tokenize.\n",
        "2. Use nltk.corpus.stopwords.\n",
        "3. Use nltk.stem.WordNetLemmatizer\n",
        "\n",
        "to convert words to their root (e.g., \"jumps\" $\\to$ \"jump\", \"transforming\" $\\to$ \"transform\").\n",
        "\n",
        "\n",
        "**Task:** Print the cleaned, lemmatized tokens for the second sentence (The pizza review)."
      ],
      "metadata": {
        "id": "dN9rNq7WycqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def clean_text_nltk(text):\n",
        "  tokens= word_tokenize(text)\n",
        "  x= stopwords.words('english')\n",
        "  for word in tokens:\n",
        "    for words in x:\n",
        "      if word==words:\n",
        "        tokens.remove(word)\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens = [lemmatizer.lemmatize(word).lower() for word in tokens]\n",
        "  return tokens\n",
        "\n",
        "\n",
        "print(clean_text_nltk(cleaned_corpus[1]))"
      ],
      "metadata": {
        "id": "v_4FjuCqy5Kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4148e5bd-1173-49e4-c77b-99d94f2c45db"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'pizza', 'absolutely', 'delicious', 'the', 'service', 'terrible', 'i', 'wont', 'go', 'back']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Text Representation**"
      ],
      "metadata": {
        "id": "hPMrwva2y1LG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1: Bag of Words (BoW)**\n",
        "\n",
        "**Logic:**\n",
        "\n",
        "**Build Vocabulary:** Create a list of all unique words in the entire corpus (after cleaning). Sort them alphabetically.\n",
        "\n",
        "**Vectorize:** Write a function that takes a sentence and returns a list of numbers. Each number represents the count of a vocabulary word in that sentence.\n",
        "\n",
        "**Task:** Print the unique Vocabulary list. Then, print the BoW vector for: \"The quick brown fox jumps over the lazy dog.\""
      ],
      "metadata": {
        "id": "cKa8NnZ5zLlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# NLTK downloads (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "arr=[]\n",
        "for i in range (0, len(corpus)):\n",
        "  arr.append(clean_text_scratch(cleaned_corpus[i]))\n",
        "\n",
        "join=[]\n",
        "for i in range (0,len(arr)):\n",
        "  join = join + arr[i]\n",
        "vocabulary=sorted(set(join))\n",
        "print(vocabulary)\n",
        "arr1=np.zeros(len(vocabulary))\n",
        "def vectorize(sentence):\n",
        "  z= clean_text_nltk(sentence)\n",
        "  for j in range (0,len(z)):\n",
        "\n",
        "\n",
        "\n",
        "    for i in range (0,len(vocabulary)):\n",
        "     if z[j].lower()==vocabulary[i]:\n",
        "      arr1[i]+=1\n",
        "  return arr1\n",
        "print(vectorize(corpus[2]))\n"
      ],
      "metadata": {
        "id": "yVUFCkm7yrg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2d24ea3-e74d-43b1-92c8-04231b02c893"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['absolute', 'algebra', 'artificial', 'back', 'be', 'behind', 'brown', 'concern', 'data', 'deliciou', 'dog', 'ethical', 'fox', 'go', 'hate', 'however', 'i', 'intelligence', 'involve', 'jump', 'lazy', 'learn', 'linear', 'love', 'machine', 'math', 'mind', 'nobler', 'not', 'over', 'pizza', 'question', 'quick', 'remain', 'science', 'service', 'statistic', 'terrible', 'that', 'the', 'ti', 'transform', 'whether', 'wont', 'world']\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2: BoW Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.CountVectorizer.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Instantiate the vectorizer.\n",
        "\n",
        "2. fit_transform the raw corpus.\n",
        "\n",
        "3. Convert the result to an array (.toarray()) and print it."
      ],
      "metadata": {
        "id": "UwsoZix-zUDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "vectorizer= CountVectorizer()\n",
        "x= vectorizer.fit_transform(corpus)\n",
        "v= x.toarray()\n",
        "df= pd.DataFrame( index=corpus, data=v, columns=vectorizer.get_feature_names_out())\n",
        "print(df)"
      ],
      "metadata": {
        "id": "RGs7EzLRzfGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d336293d-fd73-4051-aac2-1ab780b90c5a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    absolutely  algebra  and  \\\n",
            "Artificial Intelligence is transforming the wor...           0        0    0   \n",
            "The pizza was absolutely delicious, but the ser...           1        0    0   \n",
            "The quick brown fox jumps over the lazy dog.                 0        0    0   \n",
            "To be, or not to be, that is the question: Whet...           0        0    0   \n",
            "Data science involves statistics, linear algebr...           0        1    1   \n",
            "I love machine learning, but I hate the math be...           0        0    0   \n",
            "\n",
            "                                                    artificial  back  be  \\\n",
            "Artificial Intelligence is transforming the wor...           1     0   0   \n",
            "The pizza was absolutely delicious, but the ser...           0     1   0   \n",
            "The quick brown fox jumps over the lazy dog.                 0     0   0   \n",
            "To be, or not to be, that is the question: Whet...           0     0   2   \n",
            "Data science involves statistics, linear algebr...           0     0   0   \n",
            "I love machine learning, but I hate the math be...           0     0   0   \n",
            "\n",
            "                                                    behind  brown  but  \\\n",
            "Artificial Intelligence is transforming the wor...       0      0    0   \n",
            "The pizza was absolutely delicious, but the ser...       0      0    1   \n",
            "The quick brown fox jumps over the lazy dog.             0      1    0   \n",
            "To be, or not to be, that is the question: Whet...       0      0    0   \n",
            "Data science involves statistics, linear algebr...       0      0    0   \n",
            "I love machine learning, but I hate the math be...       1      0    1   \n",
            "\n",
            "                                                    concerns  ...  terrible  \\\n",
            "Artificial Intelligence is transforming the wor...         1  ...         0   \n",
            "The pizza was absolutely delicious, but the ser...         0  ...         1   \n",
            "The quick brown fox jumps over the lazy dog.               0  ...         0   \n",
            "To be, or not to be, that is the question: Whet...         0  ...         0   \n",
            "Data science involves statistics, linear algebr...         0  ...         0   \n",
            "I love machine learning, but I hate the math be...         0  ...         0   \n",
            "\n",
            "                                                    that  the  tis  to  \\\n",
            "Artificial Intelligence is transforming the wor...     0    1    0   0   \n",
            "The pizza was absolutely delicious, but the ser...     0    2    0   0   \n",
            "The quick brown fox jumps over the lazy dog.           0    2    0   0   \n",
            "To be, or not to be, that is the question: Whet...     1    2    1   2   \n",
            "Data science involves statistics, linear algebr...     0    0    0   0   \n",
            "I love machine learning, but I hate the math be...     0    1    0   0   \n",
            "\n",
            "                                                    transforming  was  \\\n",
            "Artificial Intelligence is transforming the wor...             1    0   \n",
            "The pizza was absolutely delicious, but the ser...             0    2   \n",
            "The quick brown fox jumps over the lazy dog.                   0    0   \n",
            "To be, or not to be, that is the question: Whet...             0    0   \n",
            "Data science involves statistics, linear algebr...             0    0   \n",
            "I love machine learning, but I hate the math be...             0    0   \n",
            "\n",
            "                                                    whether  won  world  \n",
            "Artificial Intelligence is transforming the wor...        0    0      1  \n",
            "The pizza was absolutely delicious, but the ser...        0    1      0  \n",
            "The quick brown fox jumps over the lazy dog.              0    0      0  \n",
            "To be, or not to be, that is the question: Whet...        1    0      0  \n",
            "Data science involves statistics, linear algebr...        0    0      0  \n",
            "I love machine learning, but I hate the math be...        0    0      0  \n",
            "\n",
            "[6 rows x 52 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3: TF-IDF From Scratch (The Math)**\n",
        "\n",
        "**Goal:** Manually calculate the score for the word \"machine\" in the last sentence:\n",
        "\n",
        "\"I love machine learning, but I hate the math behind it.\"\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "*TF (Term Frequency):* $\\frac{\\text{Count of 'machine' in sentence}}{\\text{Total words in sentence}}$\n",
        "\n",
        "*IDF (Inverse Document Frequency):* $\\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing 'machine'}})$ (Use math.log).\n",
        "\n",
        "**Result:** TF * IDF.\n",
        "\n",
        "**Task:** Print your manual calculation result."
      ],
      "metadata": {
        "id": "-MR6Bxgh0Gpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence= \"I love machine learning, but I hate the math behind it.\"\n",
        "import math\n",
        "tokens=clean_text_scratch(sentence)\n",
        "a=tokens.count(\"machine\")\n",
        "b=len(tokens)\n",
        "tf= a/b\n",
        "c= len(corpus)\n",
        "d=0\n",
        "for block in arr:\n",
        "  for word in block:\n",
        "    if word==\"machine\":\n",
        "      d=d+1\n",
        "idf = math.log(c/d)\n",
        "print(tf*idf)"
      ],
      "metadata": {
        "id": "gNSo-nza0k_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f20a5a47-bef1-44e0-a47e-753a5ae0c9a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.12206803207423442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4: TF-IDF Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.TfidfVectorizer.\n",
        "\n",
        "**Steps:** Fit it on the corpus and print the vector for the first sentence.\n",
        "\n",
        "**Observation:** Compare the score of unique words (like \"Intelligence\") vs common words (like \"is\"). Which is higher?"
      ],
      "metadata": {
        "id": "YEYkuoSb0nDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer= TfidfVectorizer()\n",
        "x = vectorizer.fit_transform([corpus[0]])\n",
        "v=x.toarray()\n",
        "df = pd.DataFrame(data=v,columns=vectorizer.get_feature_names_out())\n",
        "print(df)"
      ],
      "metadata": {
        "id": "Of6PfWyd0pnl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f941a11-93a0-4485-bf67-ecc7d740da93"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   artificial  concerns   ethical   however  intelligence        is    remain  \\\n",
            "0    0.316228  0.316228  0.316228  0.316228      0.316228  0.316228  0.316228   \n",
            "\n",
            "        the  transforming     world  \n",
            "0  0.316228      0.316228  0.316228  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3- Word Embeddings**"
      ],
      "metadata": {
        "id": "YWAar8IIzp_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1: Word2Vec Using Tools**\n",
        "\n",
        "**Task:** Train a model using gensim.models.Word2Vec.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Pass your cleaned tokenized corpus (from Part 1.2) to Word2Vec.\n",
        "\n",
        "2. Set min_count=1 (since our corpus is small, we want to keep all words).\n",
        "\n",
        "3. Set vector_size=10 (small vector size for easy viewing).\n",
        "\n",
        "**Experiment:** Print the vector for the word \"learning\"."
      ],
      "metadata": {
        "id": "uY1URFxgz036"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "model = Word2Vec(sentences=arr , min_count=1, vector_size=10)\n",
        "vector=model.wv[\"machine\"]\n",
        "print(vector)"
      ],
      "metadata": {
        "id": "aziX2IGBzyaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0375b2ee-51d6-46f8-a819-d30a5d327eb4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "[-0.07512154 -0.00930206  0.09553402 -0.07315791 -0.02334809 -0.01932827\n",
            "  0.08086619 -0.05923039  0.00043234 -0.04755129]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3: Pre-trained GloVe (Understanding Global Context)**\n",
        "\n",
        "**Task:** Use gensim.downloader to load 'glove-wiki-gigaword-50'\n",
        "\n",
        "**Analogy Task:** Compute the famous analogy:$\\text{King} - \\text{Man} + \\text{Woman} = ?$\n",
        "\n",
        "Use model.most_similar(positive=['woman', 'king'], negative=['man']).\n",
        "\n",
        "**Question:** Does the model correctly guess \"Queen\"?"
      ],
      "metadata": {
        "id": "r3J42eQZ1fUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load pre-trained GloVe model\n",
        "glove_model = api.load('glove-wiki-gigaword-50')\n",
        "\n",
        "x= glove_model.most_similar(positive=[\"woman\",\"king\"], negative=[\"man\"])\n",
        "print(x)"
      ],
      "metadata": {
        "id": "LEj5SkO81mkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d489b018-4cb5-4053-c5b1-d5893a266042"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
            "[('queen', 0.8523604273796082), ('throne', 0.7664334177970886), ('prince', 0.7592144012451172), ('daughter', 0.7473883628845215), ('elizabeth', 0.7460219860076904), ('princess', 0.7424570322036743), ('kingdom', 0.7337412238121033), ('monarch', 0.721449077129364), ('eldest', 0.7184861898422241), ('widow', 0.7099431157112122)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 5- Sentiment Analysis (The Application)**\n",
        "\n",
        "**Concept:** Sentiment Analysis determines whether a piece of text is Positive, Negative, or Neutral. We will use VADER (Valence Aware Dictionary and sEntiment Reasoner) from NLTK. VADER is specifically designed for social media text; it understands that capital letters (\"LOVE\"), punctuation (\"!!!\"), and emojis change the sentiment intensity.\n",
        "\n",
        "**Task:**\n",
        "\n",
        "1. Initialize the SentimentIntensityAnalyzer.\n",
        "\n",
        "2. Pass the Pizza Review (corpus[1]) into the analyzer.\n",
        "\n",
        "3. Pass the Math Complaint (corpus[5]) into the analyzer.\n",
        "\n",
        "**Analysis:** Look at the compound score for both.\n",
        "\n",
        "**Compound Score Range:** -1 (Most Negative) to +1 (Most Positive).\n",
        "\n",
        "Does the model correctly identify that \"delicious\" and \"terrible\" in the same sentence result in a mixed or neutral score?"
      ],
      "metadata": {
        "id": "AbI4K0UJUxy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "# Download VADER (run once)\n",
        "nltk.download('vader_lexicon')\n",
        "x= SentimentIntensityAnalyzer()\n",
        "print(x.polarity_scores(corpus[1]))\n",
        "print(x.polarity_scores(corpus[5]))"
      ],
      "metadata": {
        "id": "_lC2c3GHUxU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3294ad06-8fd6-40b6-de51-1680d4d4f8a6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.223, 'neu': 0.644, 'pos': 0.134, 'compound': -0.3926}\n",
            "{'neg': 0.345, 'neu': 0.478, 'pos': 0.177, 'compound': -0.5346}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    }
  ]
}