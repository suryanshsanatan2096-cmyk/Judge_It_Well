{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1R0wJgz48mu08Q_teQL5MgFMBjbFmP3wX","timestamp":1765207655933}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Part 1: Text Preprocessing**"],"metadata":{"id":"IYACLCwsyxfa"}},{"cell_type":"markdown","source":["\n","\n","**Data-** given in the below code cell\n","\n","**1.1: Preprocessing From Scratch**\n","\n","**Goal:** Write a function clean_text_scratch(text) that performs the following without using NLTK or Spacy:\n","\n","1. Lowercasing: Convert text to lowercase.\n","\n","2. Punctuation Removal: Use Python's re (regex) library or string methods to remove special characters (!, ., ,, :, ;, ..., ').\n","\n","3. Tokenization: Split the string into a list of words based on whitespace.\n","\n","4. Stopword Removal: Filter out words found in this list: ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'].\n","\n","5. Simple Stemming: Create a helper function that removes suffixes 'ing', 'ly', 'ed', and 's' from the end of words.\n","\n","\n","Note: This is a \"Naive\" stemmer. It will break words like \"sing\" -> \"s\". This illustrates why we need libraries!\n","\n","**Task:** Run this function on the first sentence of the corpus and print the result."],"metadata":{"id":"MTP8EqylwqDf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qIRv3qS2bTFt"},"outputs":[],"source":["corpus = [\n","    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n","    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n","    \"The quick brown fox jumps over the lazy dog.\",\n","    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n","    \"Data science involves statistics, linear algebra, and machine learning.\",\n","    \"I love machine learning, but I hate the math behind it.\"\n","]"]},{"cell_type":"code","source":["import re\n","#write rest of the code here\n","stopwords=['the','is','in','to','of','and','a','it','was','but','or']\n","def clean_text_scratch(text):\n","    text=text.lower() #lower case\n","    text = re.sub(r\"[^\\w\\s]\", \"\", text) #removing punctuation marks\n","    t=text.split() #tokenization\n","    ft = [word for word in t if word not in stopwords] # removing stopwords\n","    final=[stem(word) for word in ft] # stemming\n","    return final\n","def stem(word):\n","    if word.endswith(\"ing\"):\n","      word=word[:-3]\n","    if word.endswith(\"ly\"):\n","      word=word[:-2]\n","    if word.endswith(\"ed\"):\n","      word=word[:-2]\n","    if word.endswith(\"s\"):\n","      word=word[:-1]\n","    return word\n","corpus = [\n","    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n","    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n","    \"The quick brown fox jumps over the lazy dog.\",\n","    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n","    \"Data science involves statistics, linear algebra, and machine learning.\",\n","    \"I love machine learning, but I hate the math behind it.\"\n","]\n","a = [clean_text_scratch(corpus[0])]\n","print(a)\n","\n","\n"],"metadata":{"id":"oR4BKqITy17z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765210302034,"user_tz":-330,"elapsed":11,"user":{"displayName":"Anant Shukla","userId":"08077229075142393676"}},"outputId":"334b0722-de6e-47fe-975b-df9ba75e4b49"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['artificial', 'intelligence', 'transform', 'world', 'however', 'ethical', 'concern', 'remain']]\n"]}]},{"cell_type":"markdown","source":["**1.2: Preprocessing Using Tools**\n","\n","**Goal:** Use the nltk library to perform the same cleaning on the entire corpus.\n","\n","**Steps:**\n","\n","1. Use nltk.tokenize.word_tokenize.\n","2. Use nltk.corpus.stopwords.\n","3. Use nltk.stem.WordNetLemmatizer\n","\n","to convert words to their root (e.g., \"jumps\" $\\to$ \"jump\", \"transforming\" $\\to$ \"transform\").\n","\n","\n","**Task:** Print the cleaned, lemmatized tokens for the second sentence (The pizza review)."],"metadata":{"id":"dN9rNq7WycqZ"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('punkt_tab')\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","#write rest of the code here\n","corpus = [\n","    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n","    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n","    \"The quick brown fox jumps over the lazy dog.\",\n","    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n","    \"Data science involves statistics, linear algebra, and machine learning.\",\n","    \"I love machine learning, but I hate the math behind it.\"\n","]\n","stopwords=set(stopwords.words(\"english\"))\n","for sentence in corpus:\n"," a=word_tokenize(sentence.lower())\n"," ft = [w for w in a if w.isalpha() and w not in stopwords]\n"," b= WordNetLemmatizer()\n"," answer= [b.lemmatize(w) for w in ft]\n"," if(sentence==corpus[1]):\n","  print(answer)\n"],"metadata":{"id":"v_4FjuCqy5Kt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765369427521,"user_tz":-330,"elapsed":22,"user":{"displayName":"Anant Shukla","userId":"08077229075142393676"}},"outputId":"49e0f0e5-b67e-4af6-bb82-3b5bc5d40ee6"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["['pizza', 'absolutely', 'delicious', 'service', 'terrible', 'wo', 'go', 'back']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["# **Part 2: Text Representation**"],"metadata":{"id":"hPMrwva2y1LG"}},{"cell_type":"markdown","source":["**2.1: Bag of Words (BoW)**\n","\n","**Logic:**\n","\n","**Build Vocabulary:** Create a list of all unique words in the entire corpus (after cleaning). Sort them alphabetically.\n","\n","**Vectorize:** Write a function that takes a sentence and returns a list of numbers. Each number represents the count of a vocabulary word in that sentence.\n","\n","**Task:** Print the unique Vocabulary list. Then, print the BoW vector for: \"The quick brown fox jumps over the lazy dog.\""],"metadata":{"id":"cKa8NnZ5zLlm"}},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","# NLTK downloads (run once)\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# rest of the code here\n","corpus = [\n","    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n","    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n","    \"The quick brown fox jumps over the lazy dog.\",\n","    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n","    \"Data science involves statistics, linear algebra, and machine learning.\",\n","    \"I love machine learning, but I hate the math behind it.\"\n","]\n","#1\n","def clean(text):\n","  t=word_tokenize(text.lower())\n","  stop_words=set(stopwords.words('english'))\n","  t=[w for w in t if w not in stop_words and w.isalpha()]\n","  b=WordNetLemmatizer()\n","  t=[b.lemmatize(w) for w in t]\n","  return t\n","def unique(corpus):\n","  a=set() #set automatically removes duplicate words\n","  for sentence in corpus:\n","    a.update(clean(sentence))\n","  return sorted(list(a))\n","\n","#2\n","def vector(text,v):\n","  t=clean(text)\n","  return [t.count(word) for word in v]\n","\n"," #3\n","print(unique(corpus))\n","print(vector(corpus[2],unique(corpus)))\n","\n","\n","\n","\n"],"metadata":{"id":"yVUFCkm7yrg-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765274685659,"user_tz":-330,"elapsed":20,"user":{"displayName":"Anant Shukla","userId":"08077229075142393676"}},"outputId":"21ee8aa9-e142-4f44-bc27-64452a281497"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['absolutely', 'algebra', 'artificial', 'back', 'behind', 'brown', 'concern', 'data', 'delicious', 'dog', 'ethical', 'fox', 'go', 'hate', 'however', 'intelligence', 'involves', 'jump', 'lazy', 'learning', 'linear', 'love', 'machine', 'math', 'mind', 'nobler', 'pizza', 'question', 'quick', 'remain', 'science', 'service', 'statistic', 'terrible', 'transforming', 'whether', 'wo', 'world']\n","[0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["**2.2: BoW Using Tools**\n","\n","**Task:** Use sklearn.feature_extraction.text.CountVectorizer.\n","\n","**Steps:**\n","\n","1. Instantiate the vectorizer.\n","\n","2. fit_transform the raw corpus.\n","\n","3. Convert the result to an array (.toarray()) and print it."],"metadata":{"id":"UwsoZix-zUDC"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","#rest of the code here\n","corpus = [\n","    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n","    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n","    \"The quick brown fox jumps over the lazy dog.\",\n","    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n","    \"Data science involves statistics, linear algebra, and machine learning.\",\n","    \"I love machine learning, but I hate the math behind it.\"\n","]\n","v=CountVectorizer()\n","a=v.fit_transform(corpus)\n","b=a.toarray()\n","print(b)"],"metadata":{"id":"RGs7EzLRzfGC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765277179233,"user_tz":-330,"elapsed":114,"user":{"displayName":"Anant Shukla","userId":"08077229075142393676"}},"outputId":"3bd74eb8-fde4-454d-a7dc-1358aed2cfde"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1]\n"," [1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n","  0 0 0 0 1 0 1 0 2 0 0 0 2 0 1 0]\n"," [0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0\n","  0 1 0 0 0 0 0 0 2 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0\n","  1 0 0 0 0 0 0 1 2 1 2 0 0 1 0 0]\n"," [0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0\n","  0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0\n","  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]]\n"]}]},{"cell_type":"markdown","source":["**2.3: TF-IDF From Scratch (The Math)**\n","\n","**Goal:** Manually calculate the score for the word \"machine\" in the last sentence:\n","\n","\"I love machine learning, but I hate the math behind it.\"\n","\n","**Formula:**\n","\n","*TF (Term Frequency):* $\\frac{\\text{Count of 'machine' in sentence}}{\\text{Total words in sentence}}$\n","\n","*IDF (Inverse Document Frequency):* $\\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing 'machine'}})$ (Use math.log).\n","\n","**Result:** TF * IDF.\n","\n","**Task:** Print your manual calculation result."],"metadata":{"id":"-MR6Bxgh0Gpu"}},{"cell_type":"code","source":["# write code here\n","import math\n","corpus = [\n","    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n","    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n","    \"The quick brown fox jumps over the lazy dog.\",\n","    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n","    \"Data science involves statistics, linear algebra, and machine learning.\",\n","    \"I love machine learning, but I hate the math behind it.\"\n","]\n","c=corpus[5].lower()\n","d=[w for w in c.split()]\n","count=d.count(\"machine\")\n","tf=count/len(d)\n","i=0\n","for document in corpus:\n","  if(document.lower().count(\"machine\")!=0):\n","    i+=1\n","idf=math.log(len(corpus)/i)\n","result=tf*idf\n","print(result)\n","\n","\n","\n"],"metadata":{"id":"gNSo-nza0k_c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765281413206,"user_tz":-330,"elapsed":17,"user":{"displayName":"Anant Shukla","userId":"08077229075142393676"}},"outputId":"de1fe38c-fb0f-4be9-f62a-dcf385e2e30f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.09987384442437362\n"]}]},{"cell_type":"markdown","source":["**2.4: TF-IDF Using Tools**\n","\n","**Task:** Use sklearn.feature_extraction.text.TfidfVectorizer.\n","\n","**Steps:** Fit it on the corpus and print the vector for the first sentence.\n","\n","**Observation:** Compare the score of unique words (like \"Intelligence\") vs common words (like \"is\"). Which is higher?"],"metadata":{"id":"YEYkuoSb0nDe"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","# rest of the code here\n","corpus = [\n","    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n","    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n","    \"The quick brown fox jumps over the lazy dog.\",\n","    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n","    \"Data science involves statistics, linear algebra, and machine learning.\",\n","    \"I love machine learning, but I hate the math behind it.\"\n","]\n","t=TfidfVectorizer()\n","a=t.fit_transform(corpus)\n","b=a.toarray()\n","first=b[0]\n","import pandas as pd\n","w=pd.DataFrame([first],columns=t.get_feature_names_out())\n","d= w.loc[:,w.iloc[0] != 0] #I could think of only this via pandas\n","\n","\"\"\"score of unique words is more than common words as in unique words they appear only in a few or single sentences\n","so inside log the denominator reduces so idf value reduces and so score decreases whereas common words like is etc appear frequently in sentences so\n","the case is opposite for them . This also signifies that unique words have more significance while common words like is etc don't add much reason so their tfidf is low\"\"\"\n","d\n","\n","\n","\n","\n","\n"],"metadata":{"id":"Of6PfWyd0pnl","colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"status":"ok","timestamp":1765369186522,"user_tz":-330,"elapsed":157,"user":{"displayName":"Anant Shukla","userId":"08077229075142393676"}},"outputId":"4bf987df-b27b-4dc3-d80c-4232a9bd79de"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   artificial  concerns   ethical   however  intelligence        is    remain  \\\n","0    0.334545  0.334545  0.334545  0.334545      0.334545  0.274332  0.334545   \n","\n","        the  transforming     world  \n","0  0.171397      0.334545  0.334545  "],"text/html":["\n","  <div id=\"df-928b1891-bde9-4b73-9f0a-454c8b7648a8\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>artificial</th>\n","      <th>concerns</th>\n","      <th>ethical</th>\n","      <th>however</th>\n","      <th>intelligence</th>\n","      <th>is</th>\n","      <th>remain</th>\n","      <th>the</th>\n","      <th>transforming</th>\n","      <th>world</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.334545</td>\n","      <td>0.334545</td>\n","      <td>0.334545</td>\n","      <td>0.334545</td>\n","      <td>0.334545</td>\n","      <td>0.274332</td>\n","      <td>0.334545</td>\n","      <td>0.171397</td>\n","      <td>0.334545</td>\n","      <td>0.334545</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-928b1891-bde9-4b73-9f0a-454c8b7648a8')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-928b1891-bde9-4b73-9f0a-454c8b7648a8 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-928b1891-bde9-4b73-9f0a-454c8b7648a8');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","  <div id=\"id_8eb7405e-aa83-467c-9967-5c2599bd8868\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('d')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_8eb7405e-aa83-467c-9967-5c2599bd8868 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('d');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"d","summary":"{\n  \"name\": \"d\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"artificial\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.3345454287016015,\n        \"max\": 0.3345454287016015,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.3345454287016015\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"concerns\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.3345454287016015,\n        \"max\": 0.3345454287016015,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.3345454287016015\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ethical\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.3345454287016015,\n        \"max\": 0.3345454287016015,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.3345454287016015\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"however\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.3345454287016015,\n        \"max\": 0.3345454287016015,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.3345454287016015\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"intelligence\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.3345454287016015,\n        \"max\": 0.3345454287016015,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.3345454287016015\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.27433203727401334,\n        \"max\": 0.27433203727401334,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.27433203727401334\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"remain\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.3345454287016015,\n        \"max\": 0.3345454287016015,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.3345454287016015\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"the\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.17139656473798645,\n        \"max\": 0.17139656473798645,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.17139656473798645\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transforming\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.3345454287016015,\n        \"max\": 0.3345454287016015,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.3345454287016015\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"world\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.3345454287016015,\n        \"max\": 0.3345454287016015,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.3345454287016015\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["# **Part 3- Word Embeddings**"],"metadata":{"id":"YWAar8IIzp_m"}},{"cell_type":"markdown","source":["**3.1: Word2Vec Using Tools**\n","\n","**Task:** Train a model using gensim.models.Word2Vec.\n","\n","**Steps:**\n","\n","1. Pass your cleaned tokenized corpus (from Part 1.2) to Word2Vec.\n","\n","2. Set min_count=1 (since our corpus is small, we want to keep all words).\n","\n","3. Set vector_size=10 (small vector size for easy viewing).\n","\n","**Experiment:** Print the vector for the word \"learning\"."],"metadata":{"id":"uY1URFxgz036"}},{"cell_type":"code","source":["!pip install gensim\n","from gensim.models import Word2Vec\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","#rest of the code here\n","corpus = [\n","    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n","    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n","    \"The quick brown fox jumps over the lazy dog.\",\n","    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n","    \"Data science involves statistics, linear algebra, and machine learning.\",\n","    \"I love machine learning, but I hate the math behind it.\"\n","]#1 and 2\n","answer=[]\n","stopwords=set(stopwords.words(\"english\"))\n","for sentence in corpus:\n"," a=word_tokenize(sentence.lower())\n"," ft = [w for w in a if w.isalpha() and w not in stopwords]\n"," b= WordNetLemmatizer()\n"," answer.append([b.lemmatize(w) for w in ft])\n","model = Word2Vec(\n","    sentences=answer,\n","    vector_size=10,\n","    min_count=1,\n","    window=5,\n","    workers=2\n",")\n","\n","# 3\n","print(model.wv[\"learning\"])\n","\n"],"metadata":{"id":"aziX2IGBzyaa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765371676322,"user_tz":-330,"elapsed":6594,"user":{"displayName":"Anant Shukla","userId":"08077229075142393676"}},"outputId":"7ec28e76-a0e6-41aa-88e5-228d42fe3477"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n","Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n","[-0.00535678  0.00238785  0.05107836  0.09016657 -0.09301379 -0.07113771\n","  0.06464887  0.08973394 -0.05023384 -0.03767424]\n"]}]},{"cell_type":"markdown","source":["**3.3: Pre-trained GloVe (Understanding Global Context)**\n","\n","**Task:** Use gensim.downloader to load 'glove-wiki-gigaword-50'\n","\n","**Analogy Task:** Compute the famous analogy:$\\text{King} - \\text{Man} + \\text{Woman} = ?$\n","\n","Use model.most_similar(positive=['woman', 'king'], negative=['man']).\n","\n","**Question:** Does the model correctly guess \"Queen\"?"],"metadata":{"id":"r3J42eQZ1fUo"}},{"cell_type":"code","source":["import gensim.downloader as api\n","\n","# Load pre-trained GloVe model\n","glove_model = api.load('glove-wiki-gigaword-50')\n","\n","#rest of the code here\n","result = glove_model.most_similar(positive=[\"woman\", \"king\"], negative=[\"man\"], topn=3)\n","print(result)\n","#yes it correctly predicts queen\n","\n","\n"],"metadata":{"id":"LEj5SkO81mkF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765371298938,"user_tz":-330,"elapsed":29313,"user":{"displayName":"Anant Shukla","userId":"08077229075142393676"}},"outputId":"bd27ef0e-53c8-4278-f34c-1866228963ec"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["[('queen', 0.8523604273796082), ('throne', 0.7664334177970886), ('prince', 0.7592144012451172)]\n"]}]},{"cell_type":"markdown","source":["# **Part 5- Sentiment Analysis (The Application)**\n","\n","**Concept:** Sentiment Analysis determines whether a piece of text is Positive, Negative, or Neutral. We will use VADER (Valence Aware Dictionary and sEntiment Reasoner) from NLTK. VADER is specifically designed for social media text; it understands that capital letters (\"LOVE\"), punctuation (\"!!!\"), and emojis change the sentiment intensity.\n","\n","**Task:**\n","\n","1. Initialize the SentimentIntensityAnalyzer.\n","\n","2. Pass the Pizza Review (corpus[1]) into the analyzer.\n","\n","3. Pass the Math Complaint (corpus[5]) into the analyzer.\n","\n","**Analysis:** Look at the compound score for both.\n","\n","**Compound Score Range:** -1 (Most Negative) to +1 (Most Positive).\n","\n","Does the model correctly identify that \"delicious\" and \"terrible\" in the same sentence result in a mixed or neutral score?"],"metadata":{"id":"AbI4K0UJUxy3"}},{"cell_type":"code","source":["import nltk\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","# Download VADER (run once)\n","nltk.download('vader_lexicon')\n","# rest of the code here\n","corpus = [\n","    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n","    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n","    \"The quick brown fox jumps over the lazy dog.\",\n","    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n","    \"Data science involves statistics, linear algebra, and machine learning.\",\n","    \"I love machine learning, but I hate the math behind it.\"\n","]\n","s=SentimentIntensityAnalyzer() #1\n","pizza=corpus[1]\n","Math=corpus[5]\n","print(\"Pizza Review \", s.polarity_scores(pizza))\n","print(\"Math Complaint \", s.polarity_scores(Math))\n","\n"],"metadata":{"id":"_lC2c3GHUxU-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765371710402,"user_tz":-330,"elapsed":25,"user":{"displayName":"Anant Shukla","userId":"08077229075142393676"}},"outputId":"9607e86d-7be7-4096-9b46-20e7a1d923fe"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Pizza Review  {'neg': 0.223, 'neu': 0.644, 'pos': 0.134, 'compound': -0.3926}\n","Math Complaint  {'neg': 0.345, 'neu': 0.478, 'pos': 0.177, 'compound': -0.5346}\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n"]}]}]}